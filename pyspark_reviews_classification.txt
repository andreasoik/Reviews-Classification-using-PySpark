!pip install sparknlp
!pip install pyspark

import sparknlp
spark = sparknlp.start()
from pyspark.sql import functions as f
from google.colab import files
import io
import os
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator
from pyspark.ml.classification import NaiveBayes,DecisionTreeClassifier,GBTClassifier,RandomForestClassifier
import numpy as np
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.clustering import LDA
import nltk
import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer


def wordnet_pos(input):
    if input.startswith('J'):
            return 'a'
    elif input.startswith('V'):
            return 'v'
    elif input.startswith('N'):
            return 'n'
    elif input.startswith('R'):
            return 'r'
    else:
        return 'n'

def lemmatize_word(word_str):

    list_pos = 0
    cleaned_str = ''
    lmtzr = WordNetLemmatizer()
    text = word_str.split()
    tagged_words = nltk.pos_tag(text)
    for word in tagged_words:
        lemma = lmtzr.lemmatize(word[0], wordnet_pos(word[1]))
        if list_pos == 0:
            cleaned_str = lemma
        else:
            cleaned_str = cleaned_str + ' ' + lemma
        list_pos += 1
    return cleaned_str

spark_lemmatizer = f.udf(lambda x: lemmatize_word(x), f.StringType())

ml_df=reviews.withColumn('filtered',f.regexp_replace('text',r'[^\w\s]',' '))
ml_df=ml_df.withColumn('filtered',f.regexp_replace('filtered',r'\s*\d+\s*',' '))
ml_df=ml_df.withColumn('filtered',f.regexp_replace('filtered',r'\s+',' '))
ml_df=ml_df.withColumn('filtered',spark_lemmatizer(f.col('filtered')))
ml_df=ml_df.withColumn('filtered',f.regexp_replace('filtered','\s+\w\s+',' '))

from pyspark.ml.feature import StopWordsRemover
from pyspark.ml.feature import Tokenizer
tokenizer = Tokenizer(inputCol="filtered", outputCol="filtered_word")
ml_df = tokenizer.transform(ml_df)

# Define and apply the StopWordsRemover
remover = StopWordsRemover(inputCol="filtered_word", outputCol="final")
ml_df = remover.transform(ml_df)

# Concatenate the filtered words into a single column
ml_df = ml_df.withColumn("final", f.concat_ws(" ", "final"))
ml_df=ml_df.drop(*['filtered','filtered_word'])
ml_df.show(truncate=False)

from IPython.utils.text import num_ini_spaces
from pyspark.ml.feature import Tokenizer, HashingTF, IDF, PCA
from pyspark.ml import Pipeline

tokenizer=Tokenizer(inputCol="final", outputCol="words")
#words_data=tokenizer.transform(ml_df)


num_features=5000
hashing_tf=HashingTF(inputCol='words',outputCol='raw_features',numFeatures=num_features)
#tf_data=hashing_tf.transform(words_data)

idf=IDF(inputCol='raw_features', outputCol='tfidf_features')
#idf_model=idf.fit(tf_data)
num_components=500
pca=PCA(k=num_components,inputCol=idf.getOutputCol(),outputCol='pca_features')

pipeline=Pipeline(stages=[tokenizer,hashing_tf,idf,pca])

train_data,test_data=ml_df.randomSplit([0.7,0.3],seed=42)

model=pipeline.fit(train_data)
tfidf_train=model.transform(train_data)
tfidf_test=model.transform(test_data)
tfidf_train.show(truncate=False)

ml_df.groupBy('label').count().withColumn('prc',f.col('count')/ml_df.count()).show()

evaluator_mult=MulticlassClassificationEvaluator(labelCol='label',predictionCol='prediction',metricName='accuracy')
multiclass_metrics=['accuracy','weightedRecall','weightedPrecision','f1']

evaluator_binary=BinaryClassificationEvaluator(labelCol='label',rawPredictionCol='prediction')
binary_metrics=['areaUnderROC','areaUnderPR']

#logistic regression
log=LogisticRegression(featuresCol='pca_features',labelCol='label')
model=log.fit(tfidf_train)
predictions=model.transform(tfidf_test)

for metric in multiclass_metrics:
  result=evaluator_mult.evaluate(predictions,{evaluator_mult.metricName:metric})
  print(f'Metrics {metric}: {result:.4f}')

print('\n')
for metric in binary_metrics:
  result=evaluator_binary.evaluate(predictions,{evaluator_binary.metricName:metric})
  print(f'Binary {metric}: {result:.4f}')     

# decision tree
dtree=DecisionTreeClassifier(featuresCol='pca_features',labelCol='label')
param_grid=(ParamGridBuilder()\
            .addGrid(dtree.maxDepth, np.arange(4,11).tolist())\
            .addGrid(dtree.minInstancesPerNode, [round(tfidf_train.count()*0.01)])\
            .build())

crossval=CrossValidator(estimator=dtree, estimatorParamMaps=param_grid, evaluator=evaluator_binary, numFolds=5)

grid_search_clf=crossval.fit(tfidf_train)
clf=grid_search_clf.bestModel

predictions=clf.transform(tfidf_test)

for metric in multiclass_metrics:
  result=evaluator_mult.evaluate(predictions,{evaluator_mult.metricName:metric})
  print(f'Metrics {metric}: {result:.4f}')

print('\n')
for metric in binary_metrics:
  result=evaluator_binary.evaluate(predictions,{evaluator_binary.metricName:metric})
  print(f'Binary {metric}: {result:.4f}')       

#random forest
rf=RandomForestClassifier(featuresCol='features_clf',labelCol='label')

param_grid=(ParamGridBuilder()\
            .addGrid(rf.maxDepth, np.arange(3,11).tolist())\
            .addGrid(rf.minInstancesPerNode,[round(tfidf_train.count()*0.01)])\
            .build())

crossval=CrossValidator(estimator=rf,estimatorParamMaps=param_grid,evaluator=evaluator_binary,numFolds=5)

grid_search_clf=crossval.fit(tfidf_train)
clf=grid_search_clf.bestModel

predictions=clf.transform(tfidf_test)

for metric in multiclass_metrics:
  result=evaluator_mult.evaluate(predictions,{evaluator_mult.metricName:metric})
  print(f'Metrics {metric}: {result:.4f}')

print('\n')
for metric in binary_metrics:
  result=evaluator_binary.evaluate(predictions,{evaluator_binary.metricName:metric})
  print(f'Binary {metric}: {result:.4f}')    

#GBT classifier
gbt=GBTClassifier(featuresCol='pca_features',labelCol='label')

param_grid=(ParamGridBuilder()\
            .addGrid(gbt.maxDepth,np.arange(3,11).tolist())\
            .addGrid(gbt.minInstancesPerNode,[round(tfidf_train.count()*0.01)])\
            .build())

crossval=CrossValidator(estimator=gbt,estimatorParamMaps=param_grid,evaluator=evaluator_binary,numFolds=5)

grid_search_clf=crossval.fit(tfidf_train)
clf=grid_search_clf.bestModel

predictions=clf.transform(tfidf_test)

for metric in multiclass_metrics:
  result=evaluator_mult.evaluate(predictions,{evaluator_mult.metricName:metric})
  print(f'Metrics {metric}: {result:.4f}')

print('\n')
for metric in binary_metrics:
  result=evaluator_binary.evaluate(predictions,{evaluator_binary.metricName:metric})
  print(f'Binary {metric}: {result:.4f}')

#naive bayes
nb = NaiveBayes(featuresCol='pca_features',labelCol='label')
model=nb.fit(tfidf_train)
predictions=nb.transform(tfidf_test)

for metric in multiclass_metrics:
  result=evaluator_mult.evaluate(predictions,{evaluator_mult.metricName:metric})
  print(f'Metrics {metric}: {result:.4f}')

print('\n')
for metric in binary_metrics:
  result=evaluator_binary.evaluate(predictions,{evaluator_binary.metricName:metric})
  print(f'Binary {metric}: {result:.4f}')


# LDA
# Set LDA parameters
num_topics = 2 
lda = LDA(featuresCol='pca_features',k=num_topics, maxIter=10)

# Fit the model to the data
ldaModel = lda.fit(tfidf_train)
topics = ldaModel.describeTopics(maxTermsPerTopic=10)

pred=ldaModel.transform(tfidf_test)  